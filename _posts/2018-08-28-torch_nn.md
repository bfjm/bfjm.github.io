---
layout: post
title: torch的神经网络
date: 2018-08-28 12:00 +0800
categories: ml
tags: torch
---  

网络学习参数通过net.parameters()返回，net.named_parameters可同时返回学习参数以及名称  
params = list(net.parameters())  
print(len(params))  
print(params[0].size())   
torch.Tensor一个多维数组  
autograd.Variable包装张量并记录应用于其上的历史操作，具有tensor相同的api，还有些补充，如backward()，另外拥有张量的梯度  
nn.Module神经网络模块，方便的方式封装参数，帮助将其移动到GPU，导出，加载等  
nn.Parameter一种变量，当被指定为Model的属性时，他会自动注册一个参数  
autograd.Function实现autograd操作的向前和向后定义，每个Variable操作，至少创建一个Function节点，连接到创建Variable的函数，并编码他的历史  
net.zero_grad()     # 把之前的梯度清零  

{% highlight python linenos %}
import torch
from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
# autograd 实现了反向传播，
# 这里使用torch.nn是专门为神经网络设计的模块化接口,nn构建于autograd之上,可用来定义和运行神经网络
# nn.Module是nn中最重要的类,可以把它看成是一个网络的封装，包含网络各层定义以及forward方法，调用forward(input)方法，可返回前向传播的结果。

#一个典型的神经网络训练过程如下
# 定义具有一些可学习参数(权重)的神经网络
# 迭代输入数据集
# 通过网络处理输入
# 计算损失(输入预测值和实际值之间的距离)
# 将梯度传播回网络
# 更新网络的权重，通常使用一个简单的更新规则:weight=weight-learning_rate * gradient


class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__()
        # 卷积层 '1'表示输入图片为单通道, '6'表示输出通道数, '5'表示卷积核为5*5
        # 核心
        self.conv1 = nn.Conv2d(1, 6, 5)
        self.conv2 = nn.Conv2d(6, 16, 5)
        # 仿射层/全连接层: y = Wx + b
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        #在由多个输入平面组成的输入信号上应用2D最大池化.
        # (2, 2) 代表的是池化操作的步幅
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        # 如果大小是正方形, 则只能指定一个数字
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)
        x = x.view(-1, self.num_flat_features(x))
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

    def num_flat_features(self, x):
        size = x.size()[1:]  # 除批量维度外的所有维度
        num_features = 1
        for s in size:
            num_features *= s
        return num_features


net = Net()
print(net)  

#网络学习参数通过net.parameters()返回，net.named_parameters可同时返回学习参数以及名称
#params = list(net.parameters())
#print(len(params))
#print(params[0].size()) 



# net.zero_grad()     # 把之前的梯度清零

# print('conv1.bias.grad before backward')
# print(net.conv1.bias.grad)

# loss.backward()

# print('conv1.bias.grad after backward')
# print(net.conv1.bias.grad)
{% endhighlight %}  


 torch.Tensor一个多维数组  
 autograd.Variable包装张量并记录应用于其上的历史操作，具有tensor相同的api，还有些补充，如backward()，另外拥有张量的梯度  
 nn.Module神经网络模块，方便的方式封装参数，帮助将其移动到GPU，导出，加载等  
 nn.Parameter一种变量，当被指定为Model的属性时，他会自动注册一个参数  
 autograd.Function实现autograd操作的向前和向后定义，每个Variable操作，至少创建一个Function节点，连接到创建Variable的函数，并编码他的历史  
