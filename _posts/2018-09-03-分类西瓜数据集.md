---
layout: post
title: 西瓜分类
date: 2018-09-03 1:00 +0800
categories: ml
tags: 逻辑回归
---
TP: 预测为正，实际为正  
FP: 预测为正，实际为负  
TN:预测为负，实际为负  
FN: 预测为负，实际为正  
精确率、准确率：Accuracy=(TP+TN)/(TP+TN+FN+FP)  
精准率、查准率： P = TP/ (TP+FP)  
召回率、查全率： R = TP/ (TP+FN)  
真正例率(同召回率、查全率)：TPR = TP/ (TP+FN)  
假正例率：FPR =FP/ (FP+TN)  
F1-score: 2*TP/(2*TP + FP + FN)  


Repeat { θj := θj − α 1/m∑(i=1:m)(hθ(x(i)) − y(i) ) xj(i)
    (simultaneously update all θ j )
}

![function_h_theta]({{ site.baseurl }}/static/img/function_h_theta.png)

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

#logistic regression
import numpy as np
from sklearn.cross_validation import train_test_split
from sklearn.metrics import classification_report
# 密度
density=np.array([0.697,0.774,0.634,0.608,0.556,0.430,0.481,0.437,0.666,0.243,0.245,0.343,0.639,0.657,0.360,0.593,0.719]).reshape(-1,1)
# 含糖率
sugar_rate=np.array([0.460,0.376,0.264,0.318,0.215,0.237,0.149,0.211,0.091,0.267,0.057,0.099,0.161,0.198,0.370,0.042,0.103]).reshape(-1,1)

xtrain=np.hstack((density,sugar_rate))

xtrain=np.hstack((np.ones([density.shape[0],1]),xtrain))

ytrain=np.array([1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0]).reshape(-1,1)
# test_size 测试在测试分割中包含的数据集的比例，在0-1之间，如果没有指定，默认为0.25,random_state为随机数种子
xtrain,xtest,ytrain,ytest=train_test_split(xtrain,ytrain,test_size=0.25,random_state=33)

#sigmoid函数
def sigmoid(z):
    return 1/(1+np.exp(-z))



def logit_regression(theta,x,y,iteration=1000,learning_rate=0.1,lbd=0.01):
    for i in range(iteration):
        # 梯度下降
        theta=theta-learning_rate/y.shape[0]*(np.dot(x.transpose(),(sigmoid(np.dot(x,theta))-y))+lbd*theta)
        
        cost=-1/y.shape[0]*(np.dot(y.transpose(),np.log(sigmoid(np.dot(x,theta))))+np.dot((1-y).transpose(),np.log(1-sigmoid(np.dot(x,theta)))))+lbd/(2*y.shape[0])*np.dot(theta.transpose(),theta)
        print('Iteration %d,cost is %f'%(i,cost))
    return theta


def judge(theta,x):
    pre=np.zeros([x.shape[0],1])
    for idx,valu in enumerate(np.dot(x,theta)):
        if sigmoid(valu)>=0.5:
            pre[idx]=1
        else:
            pre[idx]=0
    return pre
                
theta_init=np.random.rand(3,1)
theta=logit_regression(theta_init,xtrain,ytrain,learning_rate=1)
pre=judge(theta,xtest)
#预测值
print('predictions are',pre)
#实际值
print('ground truth is',ytest)

print('theta is ',theta)
# 精度
print('the accuracy is',np.mean(pre==ytest))

print(classification_report(ytest,pre,target_names=['Bad','Good']))

```